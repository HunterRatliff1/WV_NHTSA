---
title: "Exploratory Data Analysis"
author: "Hunter Ratliff"
date: "`r Sys.Date()`"
params:
  remote:
    label: "Use remote data?"
    value: FALSE
output: 
  bookdown::html_document2: 
    toc: yes
    toc_float: true
    df_print: paged
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=F, message=F)

library(tidyverse)
library(here)
library(lubridate)
library(cowplot) # For multiple plots, also use gridExtra
library(ggpp) # Plotting tables on ggplots
# ggpmisc, ggpp


# For missing data
library(skimr)
library(naniar)

# For diagrams
library(DiagrammeR)

# For maps
library(sf)
library(mapdeck)
library(mapview)
library(mapboxapi)
library(mapview)
library(tmap)

library(tigris)
options(tigris_class = "sf")
options(tigris_use_cache = TRUE)

comment_header <- function(txt, width=80){
  padding <- (width - str_length(txt))/2
  padding <- floor(padding) - 5
  
  str_glue('####{str_dup("-",padding)} {txt} {str_dup("-",padding)}####')
  
}
```

# Read in data

The cleaned CSV's can be downloaded via [GitHub](https://github.com/HunterRatliff1/WV_NHTSA/tree/main/NHTSA_CSV). For downloading the actual data via the API, see [Get_NHTSA_JSON.Rmd](https://github.com/HunterRatliff1/WV_NHTSA/blob/main/Get_NHTSA_JSON.Rmd).

```{r read-csv}
if(!params$remote){
  df_cases           <- readr::read_csv(here("NHTSA_CSV", "cases.csv"), 
                                        col_types = "nnTddffdlffcfcnnnn")
  df_vehicles        <- readr::read_csv(here("NHTSA_CSV", "vehicles.csv"), 
                                        col_types = "nnnnnnlclldfcnfff") 
  df_violations_long <- readr::read_csv(here("NHTSA_CSV", "violations.csv"), 
                                        col_types = "ddddfd")
  df_persons         <- readr::read_csv(here("NHTSA_CSV", "persons.csv"), 
                                        col_types = "ddddddfffflfcfllfldf")
}
if(params$remote){
  prefix <- "https://github.com/HunterRatliff1/WV_NHTSA/raw/main/NHTSA_CSV/"
  
  df_cases           <- read_csv(paste0(prefix, "cases.csv"), 
                                 col_types = "nnTddffdlffcfcnnnn")
  df_vehicles        <- read_csv(paste0(prefix, "vehicles.csv"),
                                 col_types = "nnnnnnlclldfcnfff")
  df_violations_long <- read_csv(paste0(prefix, "violations.csv"),
                                 col_types = "ddddfd")
  df_persons         <- read_csv(paste0(prefix, "persons.csv"),
                                 col_types = "ddddddfffflfcfllfldf")
  
  rm(prefix)
} 
```

## Minor changes

We'll make a few adjustments by explicitly coding data as NA.

```{r}
# df_cases %>% skim() %>% dplyr::filter(n_missing > 0)

df_vehicles <- df_vehicles %>%
  mutate(DISTRACT = na_if(DISTRACT, "Not Reported"),
         DISTRACT = na_if(DISTRACT, "Unknown if Distracted"),
         DISTRACT = na_if(DISTRACT, "Reported as Unknown if Distracted"),
         IMPAIR   = na_if(IMPAIR, "Unknown"))
```

Additionally, because we only have have a PER_TYP of "Driver" or "Passenger" in the **persons** file, we can make this a logical flag for `driver` to more easily filter

```{r}
df_persons <- df_persons %>% 
  mutate(driver = PER_TYP=="Driver") %>%
  relocate(driver, .before = PER_TYP) 

df_persons %>%
  count(PER_TYP, driver) %>%
  knitr::kable()
```

```{r}
# df_violations_long %>%
#   group_by(CaseYear, ST_CASE, VEH_NO) %>%
#   summarise(n = unique(n_violations))
```

# Missing data

Before we get going, let's explore the missing data. We'll do this for each of the three data.frames we'll be working with:

-   Cases
-   Vehicles
-   Persons

## Cases

```{r missing-cases, class.source = 'fold-show'}
#| fig.cap = c("*Cases missing data, overall*",
#|             "*Cases missing data, by sets of missingness*")
naniar::vis_miss(df_cases, sort_miss=T)
naniar::gg_miss_upset(df_cases)
```

For the **cases** data.frame (the one with one crash per line), the main source of missing data is the time that EMS arrived (`EMS_time`) and they were transported to the hospital (`Hosp_time`). Not surprisingly, there is a strong correlation between having a missing EMS arrival time and missing hospital arrival time (the leftmost, tallest bar in the second plot above)

## Vehicles

Each row in the **vehicles** data.frame is a vehicle, so the structure here is a little bit more hairy. Some variables in this table are clearly specific to the vehicle itself (e.g. *Make/model, body*), and it doesn't look like those have many issues with missing data (Figure \@ref(fig:missing-vehicles-1), below).

However, other factors (e.g. *impairment, distraction, driver zipcode*) are driver level factors, and some of these are heavily missing. While there are some driver level factors included in this table (*if they were drinking or speeding*), one key variable, [driver death]{.underline}, isn't recorded in this table.

This could be a major confounding factor when it comes to missing data, especially for impairment, distraction, or the driver's zipcode. After all, if the driver of a motorcycle crash died immediately on impact, it may be difficult to inquire if they had been impaired by emotion or distracted.

```{r missing-vehicles, class.source = 'fold-show'}
#| fig.cap = c("*Vehicles missing data, overall*",
#|             "*Vehicles missing data, by sets of missingness*")
vis_miss(df_vehicles, sort_miss=T)
gg_miss_upset(df_vehicles) 
```

This hypothesis that some of the missing data may be coming from the driver being unavailable for questioning is somewhat supported by Figure \@ref(fig:missing-vehicles-2) above. We see that in the cases where driver zip code (`DR_ZIP`) is missing, there is often other data that are missing as well.

To explore this in more detail, we will match each vehicle's record (green box below) to their respective driver in the **persons** data.frame (red box below) and pull out if the driver died.

```{r mermaid-diagram}
DiagrammeR::mermaid("
  graph TB
    subgraph cases table
        A1[Crash 1] 
        A2[Crash 2] 
    end
    
    subgraph vehicles table
        B1[Vehicle 1]
        B2[Vehicle 2] 
        B3[...] 
    end
    
    subgraph persons table
        C(Driver<br>Vehicle 1) 
        D(Passanger A<br>Vehicle 1) 
        E(Passanger B<br>Vehicle 1) 
        F(...) 
        G(...) 
    end
    
    A1 --> B1
    A1 -.-> B2
    A2 -.-> B3
    
    B1 ==> C
    B1 --> D 
    B1 --> E
    B2 -.-> F
    B3 -.-> G
    
    classDef driver fill:#ffcccb
    classDef grey fill: #d3d3d3
    classDef green fill: #73be73
    class B1 green
    class C driver
    class B3,F,G grey
")
```

So with all that said, let's finally look at how the missing data relates to other variables in the vehicles table. This can be thought of as the "bivariate analysis" for missing data.

In figure \@ref(fig:missing-vehicles-factors) below, the rates of missingness (fill color) is plotted for each variable in the the vehicles table (Y axis) by a stratification variable (X axis)

```{r missing-vehicles-factors, fig.dim=c(8,8)}
#| fig.cap = "*Vehicles missing data, by hit & run, driver death, and year 
#| of data collection*"
p1 <- df_vehicles %>%
  gg_miss_fct(fct=HIT_RUN) +
  theme(legend.position = "bottom") +
  labs(x="Hit & Run", y="")

p2 <- df_vehicles %>%
  # Join vehicles to the driver info
  left_join(filter(df_persons, driver) %>%
              select(CaseYear, ST_CASE, VEH_NO, NUMOCCS, died),
            by = join_by(CaseYear, ST_CASE, VEH_NO, NUMOCCS)) %>%
  filter(!is.na(died)) %>%
  
  gg_miss_fct(fct=died)  +
  theme(legend.position = "bottom") +
  labs(x="Driver died", y="")

p3 <- df_vehicles %>%
  gg_miss_fct(fct=CaseYear) +
  labs(x="Case year", y="")

title <- ggdraw() + 
  draw_label(
    "Missing data for vehicles, by (A) hit & run, (B) driver death, and (C) year",
    fontface = 'bold',
    x = 0,
    hjust = 0
  ) +
  theme(
    # add margin on the left of the drawing canvas,
    # so title is aligned with left edge of first plot
    plot.margin = margin(0, 0, 0, 7)
  )

cowplot::plot_grid(
  title, 
  plot_grid(p1, p2, labels=c("A", "B")),
  plot_grid(p3, labels=c("C", "")),
  ncol=1,
  rel_heights = c(0.1, 1,.75)
)
rm(p1, p2, p3, title)
```

This reveals some interesting patterns. First, hit & runs (`HIT_RUN`) appear to account for a fair amount of the missing data that isn't impairment (`IMPAIR`) or distraction (`DSITRACT`). Second, driver death seems to account for a good portion (\~80%) of the missing impairment data.

Finally, the trends in distraction seems to change around 2018. Although the codes in the code book ([page 463](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813417#page=468)) did change slightly around that time, there don't seem to be any new definitions introduced at that time. Because this seems strange, we can take a peek of the actual data pulled from FARS.

Figure \@ref(fig:distraction-missing-graph) shows the total distractions, including missing / unknown data, for each year. For simplicity, all *specified* distractions (e.g. texting) have been categorized as "distracted". Beginning it 2018, it appears that they became more explicit in recording "not distracted" vs "not reported". To be clear, there were *some* cases prior to 2018 where drivers were "not reported", but this really looks like they changed how they reported things.

```{r distraction-missing-graph}
#| fig.cap = "*Temporal trends in missing data for distractions, including missing data*"
tempdf <- paste0("https://github.com/HunterRatliff1/WV_NHTSA/raw/main/NHTSA_CSV/", 
       "vehicles.csv") %>%
  read_csv(col_types = "nnnnnnlclldfcnfff") %>%
  mutate(Distract = case_when(
    DISTRACT == "Not Distracted" ~ "Not distracted",
    DISTRACT == "Not Reported" ~ "Not Reported",
    DISTRACT == "Unknown if Distracted" ~ "Unknown",
    DISTRACT == "Unknown if Distracted" ~ "Reported as Unknown if Distracted",
    TRUE ~ "Distracted"
  )) %>% 
  count(CaseYear, Distract) 

p1 <- tempdf %>%
  ggplot(aes(x=CaseYear, y=n, color=Distract)) + 
  geom_line() + 
  theme_half_open() +
  labs(x="Year", y="Number of cases", title="Distractions by year")
  
  
p2 <- tempdf %>% 
    pivot_wider(names_from = Distract, values_from = n) %>%
    gridExtra::tableGrob(rows=NULL) 

gridExtra::grid.arrange(p1, p2, ncol=1)
rm(p1, p2, tempdf)
```

## Persons

Finally, we're onto the **persons** data.frame. In Figure \@ref(fig:missing-persons-1), the most common missing variables are `died_timing` (which is expected) and `BAC`. `AIR_BAG` and `EJECTION` are also missing in \>1% of cases, so we will explore those as well.

```{r missing-persons, class.source = 'fold-show'}
#| fig.cap = c("*Persons missing data, overall*",
#|             "*Persons missing data, by sets of missingness*")
naniar::vis_miss(df_persons, sort_miss=T)
naniar::gg_miss_upset(df_persons)
```

We can start by tackling the `EJECTION` data in Figure \@ref(fig:ejection-missing). When we stratify this by the vehicle type[^1], we quickly see that almost 95% of these cases are appropriately marked as N/A because they were driving a motorcycle / ATV / off road vehicle.

[^1]: This requires merging the persons data.frame with the vehicles data.frame

```{r ejection-missing, fig.cap="*Missing person data by vehicle*"}
temp.df <- df_persons %>%
    left_join(df_vehicles %>%
              select(CaseYear, ST_CASE, VEH_NO, Body),
            by = join_by(CaseYear, ST_CASE, VEH_NO)) %>%
  mutate(motor = case_when(
    Body=="Motorcycle" ~ "Motorcycle",
    Body=="ATV/Off road" ~ "ATV",
    # Body=="Unknown" ~ "Unknown",
    TRUE ~ "Other"
    )) 

x <- temp.df %>%
  group_by(motor) %>%
  miss_var_summary() %>%
  filter(variable %in% c("AIR_BAG", "EJECTION")) %>%
  mutate(pct_miss = round(pct_miss, 1)) %>%
  mutate(missing = str_glue("{n_miss} ({pct_miss}%)")) %>%
  select(-n_miss, -pct_miss) %>%
  pivot_wider(names_from = variable, values_from = missing) %>%
  rename(` `=motor)
  
temp.df %>%
  gg_miss_fct(fct=motor) +
  labs(x="Vehicle type", y="", 
       title="Missing data, by vehicle type",
       subtitle = "For 'persons' table") +
  annotate(geom = 'table',
           x=2,
           y=21,
           label=list(x)) 

rm(x, temp.df)
```

```{r, eval=F, echo=F}
# Look to see if they only recorded some airbags
df_persons %>%
    left_join(df_vehicles %>%
              select(CaseYear, ST_CASE, VEH_NO, Body, HIT_RUN),
            by = join_by(CaseYear, ST_CASE, VEH_NO)) %>%
  filter(!Body %in% c("Motorcycle", "ATV/Off road")) %>%
  mutate(motor = case_when(
    Body=="Motorcycle" ~ "Motorcycle",
    Body=="ATV/Off road" ~ "ATV",
    TRUE ~ "Other"
  )) %>%
  mutate(missing_airbag = is.na(AIR_BAG)) %>% 
  
  group_by(CaseYear, ST_CASE, VEH_NO, NUMOCCS) %>%
  summarise(missing = sum(missing_airbag)) %>%
  ungroup() %>%
  count(NUMOCCS, missing) %>%
  mutate(unequal = case_when(
    missing == 0       ~ "None missing",
    NUMOCCS == missing ~ "All",
    NUMOCCS != missing ~ "Some",
  )) %>%
  group_by(unequal) %>%
  summarise(n = sum(n))
```

We can also see that these vehicles account for a fair share of the missing `AIR_BAG` data, but a good proportion (\~10%) of the missing data are not motorcycles/ATVs (Figure \@ref(fig:ejection-missing)). Additionally, there appear to be a trend in temporality, similar to what we saw in the distractions data. Figure \@ref(fig:airbag-missing) shows this trend

```{r airbag-missing, fig.cap="*Temporal trends in airbag deployment*"}
temp.df <- df_persons %>%
  mutate(AIRBAG = case_when(
    is.na(AIR_BAG) ~ "Missing",
    AIR_BAG        ~ "Deployed",
    !AIR_BAG       ~ "Not Deployed"
  )) %>%
  count(CaseYear, AIRBAG) 

temp.df %>%
  ggplot(aes(x=CaseYear, y=n, color=AIRBAG)) + 
  geom_line() + 
  theme_half_open() +
    annotate(geom = 'table',
           x=2020.5,
           y=95,
           label=list(pivot_wider(temp.df, names_from = AIRBAG, values_from = n))) + 
  labs(x="Year", y="Number of cases", title="Airbag deployment by year")
  
rm(temp.df)
```

Lastly, we need to look into the variable that had the most missing data, `BAC`. Figure \@ref(fig:bac-missing) shows that passengers are more likely to have misisng BAC levels, but drivers still have a high proportion who don't have a BAC level recorded (\~50%).

```{r bac-missing, fig.cap="*Missing person data by person type*"}
df_persons %>%
  filter(!is.na(driver)) %>%
  gg_miss_fct(fct=PER_TYP) +
    labs(x="", y="", 
       title="Missing data, by person type",
       subtitle = "For 'persons' table") 

# df_persons %>%
#   filter(!is.na(driver)) %>%
#   group_by(PER_TYP) %>%
#   miss_var_summary() %>%
#     filter(variable=="BAC")
```

# Bivaraite stuff

```{r}
df_vehicles %>%
  select(CaseYear:HIT_RUN, Body, DISTRACT, IMPAIR, totalvehicles) %>%
  left_join(select(df_cases, CaseYear, ST_CASE, LONGITUD, LATITUDE, WEATHERNAME)) %>%
  st_as_sf(coords = c("LONGITUD", "LATITUDE"), crs=6602) %>%
  ggplot() +
  geom_sf(aes(color=DR_DRINK), alpha=.15)
  # summary()
  # View()
  # glimpse()


  
```

# **Spatial Data**

Please note: The [Distance calculations](#distance-calculations) and [Drive times](#drive-times) sections contain much more technical documentation than is elsewhere in this page. Think of those sections like the methods section of an academic paper: You can read it if you're into that kind of stuff or you can skip it at your own risk

```{r download-tigris}
library(sf)
library(mapview)
library(mapboxapi)
library(mapview)
library(tmap)

library(tigris)
options(tigris_class = "sf")
options(tigris_use_cache = TRUE)

# # Find the ideal CRS for this area
# crsuggest::suggest_crs(wv_counties)
WV_crs <- "NAD83" # 6602 works well too, but must read in FARS data using NAD83

# Download geography data
wv_counties <- tigris::counties(state = "WV", cb=T) %>%
  st_transform(crs = WV_crs)
wv_tract    <- tigris::tracts(state = "WV", cb=T) %>%
  st_transform(crs = WV_crs)
wv_cbg      <- tigris::block_groups(state = "WV", cb=T) %>%
  st_transform(crs = WV_crs)
wv_roads <- tigris::primary_secondary_roads("WV")

# # Background tiles for pretty mapping
# wv_cbg_tiles <- mapboxapi::get_static_tiles(
#   location = wv_counties,
#   zoom = 7,
#   style_id = "cll13kko400rd01p4chtqcbdi",
#   username = "hunterratliff1"
# )
```

## Get local hospitals

We can download the US Department of Homeland Security's [Homeland Infrastructure Foundation-Level Data (HIFLD)](https://hifld-geoplatform.hub.arcgis.com/) hospitals [data file](https://hifld-geoplatform.opendata.arcgis.com/datasets/75079bdea94743bcaca7b6e833692639/explore). Because patients can cross the border for medical care, we'll grab all of the hospitals within 25km of West Virginia.

```{r download-hospital-locations}
# Hospital locations
hospital_url <- "https://services1.arcgis.com/Hp6G80Pky0om7QvQ/arcgis/rest/services/Hospital/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson"

hospitals <- st_read(hospital_url) %>%
  filter(STATUS=="OPEN", !TYPE %in% c("CHILDREN", "PSYCHIATRIC", "REHABILITATION", "LONG TERM CARE", "SPECIAL")) %>%
  select(ID, NAME, ADDRESS, CITY, STATE, ZIP, TYPE, NAICS_DESC, OWNER, BEDS, TRAUMA, HELIPAD) %>%
  st_transform(WV_crs) %>%
  distinct(ID, .keep_all = TRUE) %>%
  st_filter(st_transform(wv_counties, WV_crs),
            .predicate = st_is_within_distance,
            dist = 25 * 1000) #Within 25km of the border

rm(hospital_url)

# hospitals %>%
#   mapview()

# https://hifld-geoplatform.opendata.arcgis.com/maps/emergency-medical-service-ems-stations
# https://hifld-geoplatform.opendata.arcgis.com/maps/public-health-departments
# https://services1.arcgis.com/Hp6G80Pky0om7QvQ/arcgis/rest/services/Urgent_Care_Facilities/FeatureServer/0
# https://services1.arcgis.com/Hp6G80Pky0om7QvQ/arcgis/rest/services/Pharmacies/FeatureServer/0


```

```{r}
hospitals %>%
  mapview(zcol="TRAUMA")
```

## Distance calculations {#distance-calculations}

In order to calculate the distances, we can either use `sf::st_distance()` to calculate the distance directly or `mapboxapi::mb_matrix()` to calculate the driving time. However, the MapBox API limits the "maximum number of coordinates" to 25 and the number of requests to 300 per minute (five per second).

This means that something like `mb_matrix(wv_counties, hospitals)` won't work, as this would be a `r nrow(wv_counties)` by `r nrow(hospitals)` matrix. In the past, I've gotten [around similar problems](https://www.hunterratliff1.com/post/geographical-distribution-of-neurosurgeons/#find-closest-provider-for-each-county) somewhat by just finding the closest point for each polygon, but this time we're using drive times which [can be more nuanced](https://walker-data.com/census-r/spatial-analysis-with-us-census-data.html#catchment-areas-with-buffers-and-isochrones).

To accomplish this task, we will need to to take a few different steps (and make some assumptions). First, we need to narrow down our [search space](https://dba.stackexchange.com/questions/274153/what-is-search-space-when-it-comes-to-query-optimization) to find more reasonable candidate solutions. After all, we don't need to compare every location (e.g. county, location of crash) to every single hospital; instead we could calculate the travel time to the nearest couple of hospitals (instead of all `r nrow(hospitals)`) and take it from there.

The function below (**`fxn1`**) takes a set of locations[^2] (`geo_polys`) and calculates the distance matrix to each of the provided points (`geo_points`). For each `geo_polys`, it then keeps the `n_closest` number of results and returns that in a tidy style data.frame

[^2]: These can be either polygons (e.g. counties, census tracts, census block groups) or points themselves (e.g. locations of crashes). In the event that polygons are provided, it uses the distance from the centroid

```{r fxn1, class.source = 'fold-show'}
fxn1 <- function(geo_polys, geo_points, n_closest = 3,
                 name_polys="GEOID", name_points="ID"){
#' Get the distance matrix in a tidy format, and return the closest points
#' 
#' PURPOSE: For each item in `geo_polys`, find the `n_closest` `geo_points`
#'          and return that as a tidy data.frame with the distances listed
#' 
#' Calculates the distance matrix, using great circle distance, from all
#' items in `geo_polys` to all items in `geo_points`. The matrix will be 
#' quite large, nrow(geo_polys) by nrow(geo_points), so it will only return
#' the `n_closest` items
#'
#' @param geo_polys A sf object of either "MULTIPOLYGON" or "POINT" type. If
#'                  a polygon is used, distance is calculated by the centroid of
#'                  the polygon
#' @param geo_points A "POINT" sf object 
#' @param n_closest The number of items to return per geo_polys. For example
#'                  the default (5) returns the five closest geo_points for each
#'                  geo_polys
#' @param name_polys The name of the column in geo_polys to use as names. 
#'                   Defaults to "GEOID"  
#' @param name_points The name of the column in geo_points to use as names. 
#'                    Defaults to "ID"  
#' @return Tidy dataframe with three columns: name_polys, name_points, and the
#'         distance between name_polys<-->name_points
  
  
  ####--------------------------- Pre-processing ---------------------------####
  # Make the CRS match, using the poly geom
  # st_crs(geo_points) <- st_crs(geo_polys)
  # c
  # Get the type of geometries
  geo_polys_type  <- as.character(st_geometry_type(geo_polys)[[1]])
  geo_points_type <- as.character(st_geometry_type(geo_points)[[1]])
  
  # Test to see if centroids should be used (will be yes if not points)
  centroids <- geo_polys_type != "POINT"
  
  ####------------- Run some tests to check for correct inputs -------------####
    
  # inputs are sf objects
  testthat::expect_s3_class(geo_polys, "sf")
  testthat::expect_s3_class(geo_points, "sf")
  
  # Correct geometries
  testthat::expect_in(geo_polys_type, c("MULTIPOLYGON", "POINT"))
  testthat::expect_equal(geo_points_type, "POINT")
  
  # Same CRS
  testthat::expect_equal(st_crs(geo_polys)[[1]], st_crs(geo_points)[[1]])

  # Column names are in supplies data.frames
  testthat::expect_length(geo_polys[[name_polys]], nrow(geo_polys))
  testthat::expect_length(geo_points[[name_points]], nrow(geo_points))
    
  
  ####----------------------- Get the distance matrix ----------------------####
  # For each geo_polys to look up, calculate the distance to all of the 
  # geo_points. This returns a wide matrix with each row being a geo_poly
  # and column being a geo_point
  if(centroids){
    # Use the centroids
    dist <- geo_polys %>%
      st_centroid() %>%
      st_distance(geo_points)
  } 
  if(!centroids){
    dist <- geo_polys %>%
      st_distance(geo_points)
  }
  
  ####---------------------- Make it a tidy data.frame ----------------------####
  df <- dist %>%
    as.data.frame() %>%
    mutate(GEOID = geo_polys[[name_polys]]) %>%
    relocate(GEOID, .before=V1)
  
  # Rename the columns as supplied by `name_points`
  names(df) <- c(name_polys, geo_points[[name_points]])
  
  
  df %>%
    # Make long format, creating 3 columns: name_polys, name_points, and distance
    pivot_longer(cols      = -all_of(name_polys), 
                 names_to  = name_points,
                 values_to = "Distance") %>%
    
    # For each of the `geo_polys`, limit results to the `n_closest` `geo_points`
    slice_min(n        = n_closest,
              order_by = Distance,
              by       = name_polys)
}

# x <- fxn1(wv_tract, hospitals, n_closest = 5) 
```

If we set `n_closest = 1`, then this lets us calculate the distance to the closest hospital. In Figure \@ref(fig:cbg-distance) below, hospitals that are a trauma center (of any level) are shown as red dots. The color of each census block group (CBG) represents the distance in kilometers from the centroid of the CBG to the trauma center. Finally, the light black dots are the fatal crashes (from the df_cases table).

```{r cbg-distance, fig.cap="*Distance to trauma center, by CBG*"}
fxn1(wv_cbg, filter(hospitals, str_detect(TRAUMA, "LEVEL")), n_closest = 1) %>%
  # group_by(GEOID) %>%
  # filter(Distance==min(Distance)) %>%
  inner_join(wv_cbg, by = join_by(GEOID)) %>%
  mutate(Distance = as.numeric(Distance/1000)) %>%
  st_as_sf() %>%
  ggplot() +
  geom_sf(aes(fill=Distance), color=NA) +
  geom_sf(data = df_cases %>%
            st_as_sf(coords = c("LONGITUD", "LATITUDE"), crs="NAD83"), 
          alpha=0.15) +
  geom_sf(data=filter(hospitals, str_detect(TRAUMA, "LEVEL")), color="red") +
  theme_void() +
  labs(title = "Distance to trauma center, by CBG",
       subtitle = "Black dots = fatal crashes; red dots = trauma centers",
       fill="Distance (km)",
       caption="Note: Distances are from centroid of census block group\nUS Census Bureau, US Dept Homeland Security, NHTSA")
```

However, this still doesn't solve our drive time issue, which we will tackle in the next section

## Drive times {#drive-times}

We will create a similar function for the drive time, called **`fxn2`**. This is similar to [fxn1](#distance-calculations) but has a few distinctions[^3]. The most important difference is the function below calls the [MapBox API](https://docs.mapbox.com/api/navigation/matrix/). MapBox's API has [free monthly limits](https://www.mapbox.com/pricing/#directionsmatrix), and given the size of this dataset this could get very expensive. Running this function on all of the data once would cost \$52 and running it twice would cost over \$300. Hence we need to make more focused API calls, which is [addressed below](#conceptual-approach).

[^3]: An additional minor difference of fxn2 (compared to fxn1) is fxn2 doesn't limit the returned results to the n_closest results

But before we worry about that, let's explore how **`fxn2`** works:

```{r fxn2, class.source = 'fold-show'}
fxn2 <- function(case_geo, hosp_geo, sleep_time=1,
                 name_case="case_id", name_hosp="ID",
                 allow_large=TRUE){
  #' Get the drivetime matrix in a tidy format
  #'
  #' @param case_geo A "POINT" sf object of starting points
  #' @param hosp_geo A "POINT" sf object of destination points
  #' @param sleep_time How long to pause before making the API call. Used to 
  #'                   prevent timing out when making frequent API calls, 
  #'                   particularity when using this function within `map()` 
  #' @param name_case The name of the column in `case_geo` to use as IDs 
  #'                  Defaults to "case_id"  
  #' @param name_hosp The name of the column in `hosp_geo` to use as IDs 
  #'                  Defaults to "ID"
  #' @param allow_large Should the mapbox api be allowed to return large matrix?
  #'                    If true, this may get very expensive    
  #'  
  #' @return Tidy dataframe with three columns: `case_geo`, `hosp_geo`, and the
  #'         driving time between case_geo<-->hosp_geo in minutes
  
  ####------------- Run some tests to check for correct inputs -------------####
  
  # inputs are sf objects
  testthat::expect_s3_class(case_geo, "sf")
  testthat::expect_s3_class(hosp_geo, "sf")
  
  # Same CRS
  testthat::expect_equal(st_crs(case_geo)[[1]], st_crs(hosp_geo)[[1]])
  
  # Column names are in supplies data.frames
  testthat::expect_in(name_case, names(case_geo))
  testthat::expect_in(name_hosp, names(hosp_geo))
  
  ####---------------------- Get the drive time matrix ----------------------####
  # Take a pause to prevent timeouts
  Sys.sleep(sleep_time)
  
  # Call the API
  times <- mapboxapi::mb_matrix(case_geo, hosp_geo, 
                                allow_large_matrix=allow_large)
  
  ####------------------------ Create tidy dataframe ------------------------####
  times %>%
    
    ### Convert matrix to data.frame. 
    as.data.frame() %>%
    # First, we add a row to the beginning (called case.id) the ID's supplied 
    # by `name_case` column in `case_geo`.
    mutate(case.id = case_geo[[name_case]]) %>%
    relocate(case.id, .before=V1) %>%
    # Next, we rename the columns. In the matrix retruned by MapBox, V1 
    # corresponds to the first row of `hosp_geo`, V2 to the second row, etc
    magrittr::set_names( c(name_case, hosp_geo[[name_hosp]]) ) %>%
    
    
    ### Convert from wide to long format
    pivot_longer(cols      = -all_of(name_case), 
                 names_to  = name_hosp,
                 values_to = "driving") 
}
```

We can examine how **`fxn2`** works with a focused example (three crashes, two hospitals), as shown below:

```{r fxn2-example-map, cache=TRUE}
####-------------------------- Create sample data --------------------------####
temp.cases <- df_cases %>%
  mutate(case_id = str_glue("{CaseYear}_{ST_CASE}")) %>%
  filter(case_id %in% c("2018_540175", 
                        "2020_540104", 
                        # "2018_540016",
                        "2019_540210")) %>%
  st_as_sf(coords = c("LONGITUD", "LATITUDE"), crs="NAD83") %>%
  mutate(case = LETTERS[row_number()])

temp.hosp <- filter(hospitals, ID %in% c("0003326505", "0002426554")) %>%
  mutate(NAME = case_when(ID == "0003326505" ~ "WVU",
                          # ID == "0006326506" ~ "Mon General",
                          ID == "0002426554" ~ "Fairmont General"))

####------------------- Calculate distance & drive time --------------------####
temp.dist  <- fxn1(temp.cases, temp.hosp, 
                   name_polys = "case_id",
                   name_points = "NAME") %>%
  mutate(Distance = round(as.numeric(Distance)/1000,2))
temp.drive <- fxn2(temp.cases, temp.hosp, name_hosp = "NAME") %>%
  mutate(driving = round(as.numeric(driving)))

####------------------------------ Create map ------------------------------####
bind_rows(
  select(temp.hosp, ID, NAME) %>% mutate(type = "Hospital"),
  select(temp.cases, ID=case_id, NAME=case) %>% mutate(type = "Crash")
) %>% mapview(zcol="type")
```

Below is Figure \@ref(fig:fxn2-example-fig), which shows the results of calling `fxn2` for three crashes and two hospitals (in the table). The graph also shows a comparison of the drive time in relation to the great circle distance between crashes and hospital locations.

```{r fxn2-example-fig}
#| fig.cap = "*Example of how fxn2 works*"
full_join(temp.dist, temp.drive) %>%
  ggplot(aes(x=Distance, y=driving, color=case_id, shape=NAME)) +
  geom_point() +
  annotate(geom = 'table',
           x=30,
           y=0,
           label=list(pivot_wider(temp.drive, 
                                  names_from = NAME, 
                                  values_from = driving))) +
  theme_bw() +
  labs(x="Distance (km)", y="Drive time (mins)",
       title="Comparison of distance to drive time",
       subtitle = "Table shows drive time",
       shape="Hospital")

rm(temp.cases, temp.hosp, temp.dist, temp.drive)
```

### Overview of approach {#conceptual-approach}

Returning to the discussion above about limiting the search space, if we tried to find the distance for *all cases* (n=`r nrow(df_cases)`) to *all hospitals* (n=`rnrow(hospitals)`), it would be a `r nrow(df_cases)` by `r nrow(hospitals)` matrix for the travel times, which would be `r scales::comma(nrow(df_cases) * nrow(hospitals))` results!

The optimal solution would be to find the closest few hospitals (geographically) for each case and calculate the travel time on a cases-by-case basis. If we did this for the three closest hospitals, this would reduce our computational burden `r round((nrow(df_cases) * nrow(hospitals))/(nrow(df_cases)*3))`-fold. However, sending `r nrow(df_cases)` individual queries gets quite cumbersome (the API frequently times out), so I had to find another solution.

The compromise I landed on was to "chunk" things into smaller groups (in this case, by county) and do the following steps: So instead of determining the distance from *all cases* to *all hospitals*, I loop through the following [for each county]{.underline}:

A.  Split the `df_cases` table by the county the crash occurred within. We can then loop through this list of smaller cases (in the table below) and apply steps B & C to each county

B.  *For [each]{.underline} county*, use [fxn1](#distance-calculations) to find the `r formals(fxn1)[["n_closest"]]` closest hospitals for each case within the county. The list of all of these hospitals becomes the hospitals we will look at for that county

C.  *For [each]{.underline} county*, use [fxn2](#drive-times) to query the driving time from all of the cases (in that county) to the hospitals (identified in step B)

An overview of these steps is graphically shown below (TODO: Find a better package for these types of flow diagrams, maybe call the [Mermaid.js](https://mermaid.js.org/) directly instead of using it within an R package)

```{r conceptual-mermaid, fig.cap = "*Overview of the approach*", echo=F, fig.height=8}
DiagrammeR::mermaid("
  graph TB
  
    subgraph Step A
        All[<code><b>df_cases</b></code><br>1537 crashes]
        C1[Cases for<br>county 1]
        C2[Cases for<br>county 2]
        C3[Cases for<br>county <code>n</code>]
        by_county[<code><b>by_county</b></code><br>List of df]
    end
    
    All-->|split by<br>county|C1
    All-->|split by<br>county|C2
    All-.->|split by<br>county|C3
    C1-->by_county
    C2-->by_county
    C3-.->by_county
    
    hospitals[<code><b>hospitals</b></code><br>All hospitals]
    
    
    subgraph Step C...AKA...fxn3
      subgraph Step B
          fxn1(<b>Fxn1</b>) 
          hosp(Close hospitals<br><i>list of close hospitals<br>for that county</i>)
      end
      
      fxn2(<b>Fxn2</b>)
    end
    
    by_county==>|For each<br>county's<br>cases|fxn1
    hospitals-->fxn1
    fxn1-->hosp
    
    by_county==>|For each<br>county's cases|fxn2
    hosp==>fxn2
    
    %%classDef driver fill:#ffcccb
    classDef grey fill: #d3d3d3
    classDef funct fill: #73be73
    classDef global fill:#f96
    class All,hospitals global
    class fxn1,fxn2 funct
    class B,C3 grey
")
```

### Step A

For the first step, we can use the `split()` function to split the entire list of crashes (`df_cases`) by the county. This creates a list of data.frames, with one list-item for each county. We'll call this list `by_county`. The table below shows the dimensions for each of the data.frames in this list, with the first number being the number of crashes in that county

```{r split-counties}
by_county <- df_cases %>%
  mutate(case_id = str_glue("{CaseYear}_{ST_CASE}")) %>%

  # Make a list, with the df split by county
  split(df_cases$COUNTYNAME) %>%
  
  # Add the geometries
  map(\(df) st_as_sf(df, coords = c("LONGITUD", "LATITUDE"), crs="NAD83")) 
```

```{r case-county-dims, echo=FALSE}
tibble(
  dimensions = map_chr(by_county, \(x) str_glue("{nrow(x)} x {ncol(x)}"))
) %>%
  t() %>%
  as_tibble() %>%
  magrittr::set_names(str_remove(names(by_county), "\\(.+\\)"))
```

If we preview the first list-item, we can see the output is the same as if we filtered `df_cases` to `COUNTYNAME == "POCAHONTAS"`

```{r case-county-preview, collapse=T, class.source = 'fold-show'}
by_county[[1]] %>% print()

df_cases %>% 
  filter(COUNTYNAME=="POCAHONTAS (75)") %>%
  print()
```

### Step B

In the next step, we use [fxn1](#distance-calculations) to find the `r formals(fxn1)[["n_closest"]]` closest hospitals for each case within the county. The aim here is to narrow down the list of all hospitals from the global list (`hospitals`) to a more narrowed set of hospitals tailored to that counties cases. It returns a filtered versions of `hospitals`, limited to only the hospitals that had at least one match using fxn1

```{r step-b-code, eval=FALSE, class.source = 'fold-show'}
by_county %>%
  
  # For each county's cases...
  map(\(county_cases){
    # look up closest hospitals (using the list of all hospitals)
    output <- fxn1(geo_polys = county_cases,
                   geo_points = hospitals, 
                   name_polys = "case_id")
    
    # return a filtered list all hospitals, limiting to only those found above
    semi_join(hospitals, output, by = join_by(ID))
  })
```

A visual example of what we're doing here is shown below for **Webster county** (Figure \@ref(fig:step-b-example)). The big purple dots are the locations of the hospitals that are returned.

You'll note that some of the hospitals, for example *Pocahontas Memorial Hospital* (the eastern-most circle) only matched to some crashes. But since it matched to at least one crash, it's included nevertheless. This isn't the most efficient way to do things, but it still limits our search space drastically, as we now have 4 hospitals to look at for Webster county instead of `r nrow(hospitals)`.

```{r step-b-example}
#| fig.cap = "*Example of Step B for Webster county*"
# WEBSTER (101)
# HANCOCK (29)
####-------------------------- Create sample data --------------------------####
temp.cases <- by_county[["WEBSTER (101)"]] %>%
  select(case_id) 

temp.cases <- bind_cols(temp.cases,      # Make the coordinates explicit
                        st_coordinates(temp.cases)) %>%
  rename(case_lng = X, case_lat=Y) %>%
  sf::st_drop_geometry()

temp.output <- fxn1(geo_polys = by_county[["WEBSTER (101)"]], 
                  geo_points = hospitals, 
                  name_polys = "case_id") 

temp.hosp <- temp.output %>%
  semi_join(hospitals, ., by = join_by(ID)) %>%
  select(ID, NAME) 

temp.hosp <- bind_cols(temp.hosp,        # Make the coordinates explicit
                       st_coordinates(temp.hosp)) %>%
  rename(hosp_lng = X, hosp_lat=Y) %>%
  sf::st_drop_geometry()

####------------------------------ Create map ------------------------------####
library(mapdeck)
# expand_grid(temp.cases, temp.hosp) %>%
temp.output %>%
  left_join(temp.cases, by = join_by(case_id)) %>%
  left_join(temp.hosp, by = join_by(ID)) %>%

  mapdeck(pitch = 50, zoom=12) %>%
  # The arcs for distances
  add_arc(origin = c("case_lng", "case_lat"),
          auto_highlight=TRUE,
          destination = c("hosp_lng", "hosp_lat")) %>%
  
  # add points for each hospital
  add_scatterplot(data = temp.hosp,
                  lon = "hosp_lng",
                  lat = "hosp_lat",
                  tooltip = "NAME",
                  auto_highlight=TRUE,
                  radius_min_pixels = 8) %>%
  
  # add the polygon
  add_polygon(data = filter(wv_counties, GEOID=="54101"),
            fill_opacity = 0.20,
            # tooltip = "NAMELSAD",
            stroke_opacity = 1)

rm(temp.cases, temp.hosp, temp.output)
```

### Step C

In our final step, we combine the **filtered list of hospitals** from [step B] with the **county's crashes** (from the list of data.frames, `by_county`, in [step A]) to **look up the drive times** using [fxn2](#drive-times). This is a lot of moving parts, so revisiting Figure \@ref(fig:conceptual-mermaid) which shows the overview of the approach may be helpful.

Below is fxn3, which combines [fxn1](#distance-calculations) and [fxn2](#drive-times). It's designed to take a limted number of cases (`df_limited_cases`, such as one county's cases; *step A*) and an large number of places to look up (`df_hosp`, such as the hospitals data.frame). In the first part of the function, it does [step B] to limit the number of hospitals to lookup as destinations. It then calls the `mb_matrix` function (via fxn2) to look up the drive times from all cases (in `df_limited_cases`) to all hospitals identified in the aformentioned step.

Because `fxn3` will be used in a mapping[^4] function that calls the MapBox API, we don't want the loop to stop if it runs into an error (this would waste valuable API calls without returning any results). To prevent this from happening, I'll wrap the function in `purrr::possibly` which will replace errors with NA. We can then circle back to those errors later.

[^4]: That is, a `purrr::map()` function (similar to a loop), not a geographic mapping function

```{r fxn3, class.source = 'fold-show'}
fxn3 <- function(df_limited_cases, df_hosp, output="travel"){
  
  # Make sure provided output is valid. If given output of "hosp", then it 
  # only returns the output of step B
  output <- rlang::arg_match(output, c("travel", "hosp"))
  
  # This is step B: Find close hospitals
  close_hosp <- fxn1(geo_polys = df_limited_cases, 
                     geo_points = df_hosp, 
                     name_polys = "case_id") %>%
    semi_join(df_hosp, ., by = join_by(ID))
  
  if(output=="hosp") return(close_hosp)
  
  # This is step C: Look up travel times
  fxn2(case_geo = df_limited_cases, 
       hosp_geo = close_hosp, 
       allow_large = FALSE)
}

# fxn3(by_county[[1]], hospitals)


## Create version of fxn3 that won't stop the `map()` if one of the counties fail
fxn3_possible <- purrr::possibly(fxn3, NA_real_)
```

Now that we've finally outlined the methodology, we can test it to see how much it narrowed down the search space (compared to looking at the drive time from all `r nrow(df_cases)` cases to all `r nrow(hospitals)` hospitals.

Figure \@ref(fig:show-matrix-size) shows the rection in this computational burden. The grey box shows the original size of the distance/drive-time matrix, and the colored boxes show the matrix size for each county's cases

```{r show-matrix-size, cache=TRUE}
#| fig.cap = "*Size of matrices using my approach vs original*"
####-------------------------- Create sample data --------------------------####
temp.df <- tibble(county  = names(by_county),
       crashes = map_int(by_county, nrow),
       hospitals = map_int(by_county, \(df){
         fxn3(df, df_hosp=hospitals, output="hosp") %>%
          nrow()
       }, .progress = T)) %>%
  mutate(n_matrix = crashes * hospitals)

# Prepare format for plotting
temp.df <- temp.df %>% 
  arrange(n_matrix) %>%
  mutate(crashes_max = cumsum(crashes),
         crashes_min = lag(crashes_max,default = 0)+1) %>%
  
  # # Select cases to annotate
  # mutate(anno = n_matrix>=temp.cutoff) %>%
  # mutate(anno = hospitals>=7) %>%
  mutate(anno = row_number() %% 5 == 0) %>%
  mutate(anno = ifelse(anno, str_glue("{crashes} x {hospitals}"), NA))


temp.df %>%
  mutate(fill=row_number() %% 6) %>%
  ggplot(aes(xmin = 0,         ymin = crashes_min, 
             xmax = hospitals, ymax = crashes_max)) +
    
  # Make a annotation with the original size of the matrix
  annotate("rect", xmin = 0, xmax = nrow(hospitals), 
           ymin = 0, ymax = nrow(df_cases),
           alpha = .2, fill="grey", color="black") +
   annotate("text", x = 70, y=1400, 
            label = str_glue("Original matrix:\n{nrow(df_cases)} x {nrow(hospitals)}")) +
    
  geom_rect(aes(fill=factor(fill))) +
  ggrepel::geom_label_repel(aes(x=hospitals, y=crashes_max, label=anno),
                            # nudge_y = 50, 
                            nudge_x =10) +
  labs(x="", y="", fill="Matrix size", 
       title="Search space",
       subtitle = "Grey box is original search space; each colored box is the matrix for each county") +
  # cowplot::theme_nothing() +
  theme_void() +
  scale_fill_brewer(type="qual") + 
  # coord_fixed(ratio=.1) +
  guides(fill="none")
  
rm(temp.df)
```

### Call MapBoxAPI

```{r load-travel-times}
# This is to be run if we aren't actually downloading the data
rm(by_county)

if(!params$remote){
  travel <- here("NHTSA_CSV", "traveltime-all-hospitals.csv") %>%
    readr::read_csv(col_types = "ddcd")
}
if(params$remote){
  travel <- paste0("https://github.com/HunterRatliff1/",
    "WV_NHTSA/raw/main/NHTSA_CSV/",
    "traveltime-all-hospitals.csv") %>%
    readr::read_csv(col_types = "ddcd")
} 
```

Now we can finally actually do the API calls and get the drive times!

There are a couple counties that fail (due to [a bug](https://github.com/walkerke/mapboxapi/issues/42) with `mapboxapi::mb_matrix`), so we have to make two passes: first we try all counties, then we circle back to the ones who failed. Once we've gotten all of the data from the API, we join all the data together in one file and write it to a CSV

```{r travel_times-all, eval=FALSE, message=TRUE, class.source = 'fold-show'}
# Try to get the counties that we can (some will fail)
travel <- by_county %>%
    map(\(df) fxn3_possible(df, df_hosp=hospitals), .progress = T)

# These are the ones that we could look up
temp.df <- travel %>% 
  discard(\(x) length(x)==1) %>%
  dplyr::bind_rows()

# now we can look up the remaining cases (treating them as if they were
# all in the same county)... this gets around the bug
temp.df2 <- df_cases %>%
  mutate(case_id = str_glue("{CaseYear}_{ST_CASE}")) %>%
  st_as_sf(coords = c("LONGITUD", "LATITUDE"), crs="NAD83") %>%
  anti_join(temp.df, join_by(case_id)) %>%
  fxn3(df_hosp=hospitals)

# # Verify we've looked up all cases
# df_cases %>%
#   mutate(case_id = str_glue("{CaseYear}_{ST_CASE}")) %>%
#   anti_join(dplyr::bind_rows(temp.df, temp.df2))

# Write to file
dplyr::bind_rows(temp.df, temp.df2) %>% 
  tidyr::separate_wider_delim(case_id, 
                              delim="_", 
                              names = c("CaseYear", "ST_CASE")) %>%
  rename(hospital_ID = ID) %>%
  readr::write_csv(here("NHTSA_CSV", "traveltime-all-hospitals.csv"))

rm(by_county, travel, temp.df, temp.df2)

```

### TODO: Same thing but only trauma centers 

```{r, eval=F}
temp.df <- df_persons %>%
  left_join(select(df_cases, CaseYear, ST_CASE, 
                   LONGITUD, LATITUDE, WEATHERNAME)) %>%
  filter(str_detect(HOSPITAL, "EMS")) %>%
  st_as_sf(coords = c("LONGITUD", "LATITUDE"), crs=WV_crs)

wv_counties %>% 
  ggplot() +
  geom_sf() + 
  geom_sf(aes(color=HOSPITAL), alpha=.25, data=temp.df) +
  facet_wrap("HOSPITAL")

rm(temp.df)
```

```{r sessioninfo}
sessioninfo::session_info()
```
